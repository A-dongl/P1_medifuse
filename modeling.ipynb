{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1dfbd68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T12:04:02.648251Z",
     "start_time": "2022-10-27T12:03:25.229008Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Byeonghoon\\AppData\\Local\\Temp\\ipykernel_12636\\707501045.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dt1.drop(a, axis=0, inplace=True)\n",
      "C:\\Users\\Byeonghoon\\AppData\\Local\\Temp\\ipykernel_12636\\707501045.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dt1.drop(b, axis=0, inplace=True)\n",
      "C:\\Users\\Byeonghoon\\AppData\\Local\\Temp\\ipykernel_12636\\707501045.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  daataa[i] = daataa[i].replace(k, ' ')\n",
      "C:\\anaconda3\\envs\\bhoon\\lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n",
      "C:\\anaconda3\\envs\\bhoon\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9997137122332559\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "dt = pd.read_csv(\"TrainSet _1st.csv\")\n",
    "dt.columns = ['F','C','Y']\n",
    "\n",
    "dt = dt.drop_duplicates(subset = None, keep= 'first', inplace= False, ignore_index= True)\n",
    "dt.fillna(\"\", inplace=True)\n",
    "dt['New'] = dt['F'] + \"\\n\" + dt['C']\n",
    "dt1 = dt[['New', 'Y']]\n",
    "\n",
    "\n",
    "dt_ch_text=dt1[dt1['New'].str.contains('no specific interval change compare to the latest MR brain', na=False)]\n",
    "a = dt_ch_text[dt_ch_text['Y'] != 0].index\n",
    "dt_ch1_text=dt1[dt1['New'].str.contains('No evidence of acute infarction', na=False)]\n",
    "b= dt_ch1_text[dt_ch1_text['Y'] != 0].index\n",
    "dt1.drop(a, axis=0, inplace=True)\n",
    "dt1.drop(b, axis=0, inplace=True)\n",
    "\n",
    "def howmax(text):\n",
    "  li = re.findall(\"\\n[0-9]+[.] \", re.sub(\" *\\n *\", \"\\n\", text))\n",
    "  li = [re.sub(\"[^\\d+]\", \"\", i) for i in li]\n",
    "\n",
    "  if len(li) != 0:\n",
    "    li = [int(i) for i in li]\n",
    "    res = max(li)\n",
    "    return res\n",
    "\n",
    "num = max(dt1['New'].map(str).apply(lambda x:howmax(x)))\n",
    "\n",
    "dt1 = dt1.replace('\\r', ' ', regex=True)\n",
    "dt1 = dt1.replace('Clinical information','', regex=True)\n",
    "dt1 = dt1.replace('CI','', regex=True)\n",
    "dt1 = dt1.replace('조영증강은 시행하지 않았음', '', regex=True)\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "  text_rmv = re.sub(\"[^a-zA-Z가-힣0-9\\n +/.]\", \" \", text)\n",
    "  text_rmv = re.sub(\"\\n+\", \"\\n\", text_rmv)\n",
    "  text_rmv = re.sub(\" +\", \" \", text_rmv)\n",
    "  text_rmv = ' '.join(text_rmv.split())\n",
    "\n",
    "  return text_rmv\n",
    "\n",
    "dt1['New'] = dt1['New'].map(str).apply(lambda x: clean_text(x))\n",
    "\n",
    "emlist = []\n",
    "def delnum(bb):\n",
    "  for i in range(1,int(bb)+1):\n",
    "    soot = str(i) + str('. ')\n",
    "    emlist.append(soot)\n",
    "  return emlist\n",
    "\n",
    "final_num_list = delnum(num)\n",
    "final_num_list\n",
    "\n",
    "def seee(daataa, f_num_l):\n",
    "\n",
    "  for i in daataa.index.tolist():\n",
    "\n",
    "    for k in f_num_l:\n",
    "      daataa[i] = daataa[i].replace(k, ' ')\n",
    "    \n",
    "  return daataa\n",
    "\n",
    "dt1['New'] = seee(dt1['New'], final_num_list)\n",
    "\n",
    "from ckonlpy.tag import Twitter\n",
    "\n",
    "twitter = Twitter()\n",
    "twitter.add_dictionary(['조영증강', 'T1WI', 'T2WI', 'T1', 'T2'], 'Noun')\n",
    "\n",
    "def tw_tokenizer(text):\n",
    "  tokens_ko = twitter.morphs(text)\n",
    "  return tokens_ko \n",
    "\n",
    "\n",
    "X = dt1['New']\n",
    "y = dt1['Y']\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=tw_tokenizer, ngram_range=(1,3),min_df=2, max_df=0.9)\n",
    "tfidf.fit(X)\n",
    "\n",
    "X_train_tfidf = tfidf.transform(X)\n",
    "X_test_tfidf = tfidf.transform(X)\n",
    "\n",
    "lr = LogisticRegression(C=10, max_iter=80)\n",
    "lr.fit(X_train_tfidf, y)\n",
    "pred = lr.predict(X_test_tfidf)\n",
    "pred_probs = lr.predict_proba(X_test_tfidf)[:,1]\n",
    "\n",
    "auroc = roc_auc_score(y, pred_probs)\n",
    "print(auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6c79be4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T12:41:57.897677Z",
     "start_time": "2022-10-27T12:41:57.883419Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79766449",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
